{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4: Algoritmos de aprendizaje supervisado clásicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este tutorial, describemos los principios de distintos algoritmos clásicos de aprendizaje supervisado: \n",
    "- K Vecinos más cercanos\n",
    "- Naïve Bayes\n",
    "- Árbol de decisión\n",
    "- Ensemble: Random Forest, AdaBoost\n",
    "\n",
    "Ilustramos su uso con una tarea de análisis de sentimientos con el dataset IMDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparación del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"datos/imdb.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>count(*)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment  count(*)\n",
       "0  positive     25000\n",
       "1  negative     25000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandasql import sqldf\n",
    "\n",
    "q=\"\"\"SELECT sentiment, count(*) FROM df GROUP BY sentiment ORDER BY count(*) DESC;\"\"\"\n",
    "result=sqldf(q)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=\"\"\"SELECT * FROM df WHERE sentiment = \"positive\";\"\"\"\n",
    "df_positive=sqldf(q)\n",
    "\n",
    "df_positive = df_positive.sample(n=1000)\n",
    "\n",
    "q=\"\"\"SELECT * FROM df WHERE sentiment = \"negative\";\"\"\"\n",
    "df_negative=sqldf(q)\n",
    "\n",
    "df_negative = df_negative.sample(n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df_positive, df_negative], ignore_index=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(text):\n",
    "    \n",
    "    mytokens = nlp(text)\n",
    "\n",
    "    #Guardamos las palabras como características si corresponden a ciertas categorias gramaticales\n",
    "    mytokens = [ word for word in mytokens if word.pos_ in [\"NOUN\", \"ADJ\", \"VERB\"] ]\n",
    "    \n",
    "    #Transformamos las palabras en minusculas\n",
    "    mytokens = [ word.lemma_.lower().strip() for word in mytokens ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vector = TfidfVectorizer(tokenizer = feature_extraction, min_df=0., max_df=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['review'] \n",
    "ylabels = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    positive\n",
       "1    positive\n",
       "2    positive\n",
       "3    positive\n",
       "4    positive\n",
       "Name: sentiment, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ylabels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb = LabelEncoder()\n",
    "ylabels_encoded = lb.fit_transform(ylabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ylabels_encoded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels_encoded, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. K vecinos más cercanos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "n_neighbors = 15\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "\n",
    "model_knn = Pipeline([('vectorizing', tfidf_vector),\n",
    "                 ('learning', knn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiempo para entrenar:58.915780544281006\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model_knn.fit(X_train, y_train)\n",
    "\n",
    "end = time.time()\n",
    "print(\"tiempo para entrenar:\" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiempo para predecir:56.45247983932495\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "predicted = model_knn.predict(X_test)\n",
    "\n",
    "end = time.time()\n",
    "print(\"tiempo para predecir:\" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[384 124]\n",
      " [139 353]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.76      0.74       508\n",
      "           1       0.74      0.72      0.73       492\n",
      "\n",
      "    accuracy                           0.74      1000\n",
      "   macro avg       0.74      0.74      0.74      1000\n",
      "weighted avg       0.74      0.74      0.74      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test, predicted)\n",
    "\n",
    "print(confusion_matrix)\n",
    "\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Descripción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/knn.png\" />\n",
    "\n",
    "\n",
    "Ventajas:\n",
    "\n",
    "- Simple: número de vecinos, distancia entre vectores (Euclidiana, Manhattan)\n",
    "- No requiere entrenamiento (_lazy_)\n",
    "- Interesante para resolver problemas de clasificación de datos no linealmente separables\n",
    "\n",
    "Desventajas:\n",
    "\n",
    "- En la fase de predicciones, el rendimiento baja si el dataset de entrenamiento es grande. No hay generalización, se necesita calcular distancias con cada instancia.\n",
    "- Si las caracteristicas utilizan escalas distintas, impactar mucho el calculo de la distancia (se requiere normalizar)\n",
    "- Requiere memoria\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "model_nb = Pipeline([('vectorizing', tfidf_vector),\n",
    "                 ('learning', nb)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiempo para entrenar:54.50274157524109\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model_nb.fit(X_train, y_train)\n",
    "\n",
    "end = time.time()\n",
    "print(\"tiempo para entrenar:\" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "predicted = model_nb.predict(X_test)\n",
    "\n",
    "end = time.time()\n",
    "print(\"tiempo para predecir:\" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, predicted)\n",
    "\n",
    "print(confusion_matrix)\n",
    "\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_salient_words(nb_clf, vect, class_ind):\n",
    "    \"\"\"Return salient words for given class\n",
    "    Parameters\n",
    "    ----------\n",
    "    nb_clf : a Naive Bayes classifier (e.g. MultinomialNB, BernoulliNB)\n",
    "    vect : CountVectorizer\n",
    "    class_ind : int\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        a sorted list of (word, log prob) sorted by log probability in descending order.\n",
    "    \"\"\"\n",
    "\n",
    "    words = vect.get_feature_names()\n",
    "    zipped = list(zip(words, nb_clf.feature_log_prob_[class_ind]))\n",
    "    sorted_zip = sorted(zipped, key=lambda t: t[1], reverse=True)\n",
    "\n",
    "    return sorted_zip\n",
    "\n",
    "neg_salient_top = get_salient_words(nb, tfidf_vector, 0)[:10]\n",
    "pos_salient_top = get_salient_words(nb, tfidf_vector, 1)[:10]\n",
    "\n",
    "print(neg_salient_top)\n",
    "print(pos_salient_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Descripción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teorema de Bayes:\n",
    "\n",
    "<img src=\"img/naivebayes.jpeg\" />\n",
    "\n",
    "**Modelo probabilístico que asume que existe una independencia entre las caractéristicas.**\n",
    "\n",
    "\n",
    "¿Qué se aprende?: probabilidades\n",
    "\n",
    "\n",
    "Ventajas:\n",
    "\n",
    "- Cuando la suposición independiente se mantiene, entonces este clasificador da una precisión excepcional.\n",
    "- Es fácil de implementar ya que sólo se debe calcular la probabilidad\n",
    "- Funciona bien con dimensiones altas como la clasificación de texto.\n",
    "\n",
    "Desventajas:\n",
    "\n",
    "- Si la suposición independiente no se mantiene, entonces el rendimiento es muy bajo.\n",
    "\n",
    "- Suavizar resulta ser un paso obligado cuando la probabilidad de una característica resulta ser cero en una clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Árbol de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "my_tree = tree.DecisionTreeClassifier()\n",
    "\n",
    "model_tree = Pipeline([('vectorizing', tfidf_vector),\n",
    "                 ('learning', my_tree)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model_tree.fit(X_train, y_train)\n",
    "\n",
    "end = time.time()\n",
    "print(\"tiempo para entrenar:\" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "predicted = model_tree.predict(X_test)\n",
    "\n",
    "end = time.time()\n",
    "print(\"tiempo para predecir:\" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, predicted)\n",
    "\n",
    "print(confusion_matrix)\n",
    "\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Source\n",
    "graph = Source( tree.export_graphviz(my_tree, out_file=None, feature_names=tfidf_vector.get_feature_names()))\n",
    "graph.format = 'png'\n",
    "graph.render('tree_render',view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_representation = tree.export_text(my_tree, feature_names=tfidf_vector.get_feature_names(), max_depth=10)\n",
    "print(txt_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Descripción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué se aprende?: un arbol de preguntas para dividir los datos de entrenamiento.\n",
    "    \n",
    "<img src=\"img/CART.png\"></img> \n",
    "\n",
    "En nuestro ejemplo, el nodo 1 está totalmente desenredado (label: \"Grape\"). El nodo 2 tiene dos labeles, entonces preguntamos otra pregunta:\n",
    "\n",
    "<img src=\"img/CART2.png\"></img>\n",
    "\n",
    "Para construir un árbol eficiente, el punto importante es identificar qué preguntas formular y cuándo. Por lo tanto, necesitamos <b>cuantificar</b> en qué medida una pregunta permite desenredar los labeles. Para hacer eso se utiliza 2 métricas:\n",
    "- el coeficiente de '<b>Gini impurity</b>': mide que tan desenredados están los labeles de un nodo.\n",
    "- el coeficiente de '<b>Information Gain</b>': mide cuánto una pregunta permite bajar el 'Gini impurity'.\n",
    "\n",
    "<img src=\"img/CART3.png\"></img>\n",
    "\n",
    "Utilizaremos estas métricas para estimar qué preguntas hacer.\n",
    "\n",
    "<img src=\"img/CART4.png\"></img>\n",
    "\n",
    "Para saber qué preguntas formular, cada nodo itera sobre las características de los datos a su disposición y define una lista de preguntas posibles.\n",
    "\n",
    "Una vez una pregunta elegida, se divide los datos en dos según la respuesta a la pregunta.\n",
    "\n",
    "<img src=\"img/CART5.png\"></img>\n",
    "\n",
    "El coeficiente de Gini impurity representa la probabilidad de ser incorrecto si asigna aleatoriamente una etiqueta a un ejemplo del mismo conjunto. Por ejemplo, en los dos ejemplos siguientes: ¿Cuál es la probabilidad de equivocarse si asignamos una etiqueta del recipiente B a un dato del recipiente A?\n",
    "\n",
    "Ejemplo 1:\n",
    "<img src=\"img/CART-6.png\"></img>\n",
    "\n",
    "Ejemplo 2:\n",
    "<img src=\"img/CART-7.png\"></img>\n",
    "\n",
    "\n",
    "La métrica de **Information Gain** permite medir qué pregunta optimiza el coeficiente de Gini impurity.\n",
    "\n",
    "Por cada nodo, empezamos por medir el coeficiente de Gini impurity de los labeles disponibles. Luego, por cada pregunta calculamos el coeficiente de Gini impurity de los dos sub-conjuntos de datos obtenidos.\n",
    "\n",
    "<img src=\"img/CART-8.png\"></img>\n",
    "\n",
    "Calculamos la incerteza (<i>impurity</i>) promedia ponderada para los dos subconjuntos de datos obtenidos. Por ejemplo:\n",
    "\n",
    "<img src=\"img/CART-9.png\"></img>\n",
    "\n",
    "Finalmente, conservamos la pregunta que permite optimizar la ganancia de información (Information Gain). En nuestro ejemplo:\n",
    "\n",
    "<b>Information Gain = 0.64 - 0.50 = 0.14</b>\n",
    "\n",
    "\n",
    "Ventajas:\n",
    "\n",
    "- es interpretable e intuitivo\n",
    "- no requiere normalizar o re-escalar los datos\n",
    "- los datos incompletos no impactan el entrenamiento\n",
    "\n",
    "\n",
    "Desventajas:\n",
    "\n",
    "- pequeños cambios en el dataset de entrenamiento puede generar grandes impactos en la estructura del árbol\n",
    "- overfitting (modelo demasiado ajustado al dataset de training)\n",
    "- costoso en computación en la fase de entrenamiento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=50, max_depth=None, min_samples_split=10, random_state=0)\n",
    "\n",
    "model_random_forest = Pipeline([('vectorizing', tfidf_vector),\n",
    "                 ('learning', random_forest)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model_random_forest.fit(X_train, y_train)\n",
    "\n",
    "end = time.time()\n",
    "print(\"tiempo para entrenar:\" + str(end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "predicted = model_random_forest.predict(X_test)\n",
    "\n",
    "end = time.time()\n",
    "print(\"tiempo para predecir:\" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, predicted)\n",
    "\n",
    "print(confusion_matrix)\n",
    "\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ada Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada = AdaBoostClassifier(n_estimators=100)\n",
    "\n",
    "model_ada = Pipeline([('vectorizing', tfidf_vector),\n",
    "                 ('learning', ada)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ada.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model_ada.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, predicted)\n",
    "\n",
    "print(confusion_matrix)\n",
    "\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Descripción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combinar varios clasificadores debiles:\n",
    "\n",
    "<img src=\"img/ensemble.png\" />\n",
    "\n",
    "\n",
    "Funcionamiento detallado de RandomForest: https://www.youtube.com/watch?v=J4Wdy0Wc_xQ (20 minutos)\n",
    "\n",
    "Funcionamento detallado de AdaBoost: https://www.youtube.com/watch?v=LsK-xG1cLYA (20 minutos)\n",
    "\n",
    "\n",
    "Ventajas:\n",
    "\n",
    "- Reduce el overfitting de los arboles de decisión\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
